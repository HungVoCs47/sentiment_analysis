{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4639,"status":"ok","timestamp":1653989252196,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"aoVjFKmOZMZ8","outputId":"bcd119ff-e091-4cf5-ca96-45e131211e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["\n","Requirement already satisfied: pyvi in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.1)\n","Requirement already satisfied: scikit-learn in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyvi) (1.3.1)\n","Requirement already satisfied: sklearn-crfsuite in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyvi) (0.3.6)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->pyvi) (1.26.0)\n","Requirement already satisfied: scipy>=1.5.0 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->pyvi) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->pyvi) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->pyvi) (3.2.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.9)\n","Requirement already satisfied: six in c:\\users\\vho8hc\\appdata\\roaming\\python\\python311\\site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n","Requirement already satisfied: tabulate in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n","Requirement already satisfied: tqdm>=2.0 in c:\\users\\vho8hc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\vho8hc\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.6)\n"]}],"source":["!pip install pyvi"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8TFX7wSAmg9y"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\VHO8HC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.17) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n","  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"]}],"source":["import tensorflow as tf\n","import pandas as pd \n","import numpy as np\n","from string import digits\n","from collections import Counter\n","from pyvi import ViTokenizer\n","from gensim.models.word2vec import Word2Vec\n","from tensorflow.keras.utils import to_categorical\n","%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"a7lMy03omg93","scrolled":true},"outputs":[],"source":["data_train = pd.read_csv(\"data/vlsp_sentiment_train.csv\", sep='\\t')\n","data_train.columns =['Class', 'Data']\n","data_test = pd.read_csv(\"data/vlsp_sentiment_test.csv\", sep='\\t')\n","data_test.columns =['Class', 'Data']"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":944,"status":"ok","timestamp":1653989301990,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"4HR1jAzImg94","outputId":"8e324c8c-0dbb-4a69-aeff-b03e40766c7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5100, 2)\n","(1050, 2)\n"]}],"source":["print(data_train.shape)\n","print(data_test.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jvrbwPfZmg95"},"outputs":[],"source":["labels = data_train.iloc[:, 0].values\n","reviews = data_train.iloc[:, 1].values"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3HlbVeHimg95"},"outputs":[],"source":["encoded_labels = []\n","\n","for label in labels:\n","    if label == -1:\n","        encoded_labels.append([1,0,0])\n","    elif label == 0:\n","        encoded_labels.append([0,1,0])\n","    else:\n","        encoded_labels.append([0,0,1])\n","\n","encoded_labels = np.array(encoded_labels)  "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Lm4OCwxXmg96"},"outputs":[],"source":["reviews_processed = []\n","unlabeled_processed = [] \n","for review in reviews:\n","    review_cool_one = ''.join([char for char in review if char not in digits])\n","    reviews_processed.append(review_cool_one)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nW2OZgkgmg97"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews = []\n","all_words = []\n","for review in reviews_processed:\n","    review = ViTokenizer.tokenize(review.lower())\n","    word_reviews.append(review.split())\n","   "]},{"cell_type":"code","execution_count":9,"metadata":{"id":"pTb0MeDRmg98"},"outputs":[],"source":["EMBEDDING_DIM = 400 # how big is each word vector\n","MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n","MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"jW-7mKtWmg9-"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-BHpPSLTmg9_"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews)\n","word_index = tokenizer.word_index\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"LlV3M2dimg9_"},"outputs":[],"source":["data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = encoded_labels"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653989306259,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"4dl9VZ3Rmg-A","outputId":"0d43e170-e903-4d5b-f3ec-18a8b911d072"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (5100, 300)\n","Shape of label train and validation tensor: (5100, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data.shape)\n","print('Shape of label train and validation tensor:', labels.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-KKSjJdJmg-A"},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","word_vectors = KeyedVectors.load_word2vec_format('vi-model-CBOW.bin', binary=True)\n","\n","\n","vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n","embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i>=MAX_VOCAB_SIZE:\n","        continue\n","    try:\n","        embedding_vector = word_vectors[word]\n","        embedding_matrix[i] = embedding_vector\n","    except KeyError:\n","        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","del(word_vectors)\n","\n","from keras.layers import Embedding\n","embedding_layer = Embedding(vocabulary_size,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            trainable=True)"]},{"cell_type":"markdown","metadata":{},"source":["#LSTM MODEL"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 300)]             0         \n","                                                                 \n"," embedding (Embedding)       (None, 300, 400)          3168000   \n","                                                                 \n"," reshape (Reshape)           (None, 300, 400)          0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 300, 1024)         5836800   \n","                                                                 \n"," lstm_2 (LSTM)               (None, 300, 512)          3147776   \n","                                                                 \n"," lstm_3 (LSTM)               (None, 256)               787456    \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 3)                 771       \n","                                                                 \n","=================================================================\n","Total params: 12940803 (49.37 MB)\n","Trainable params: 12940803 (49.37 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["from keras.models import Model\n","from keras.layers import *\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Model\n","from keras import regularizers\n","sequence_length = data.shape[1]\n","filter_sizes = [3,4,5]\n","num_filters = 100\n","drop = 0.5\n","\n","inputs = Input(shape=(sequence_length,))\n","embedding = embedding_layer(inputs)\n","\n","\n","reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","\n","lstm_0 = LSTM(512)(reshape)\n","\n","lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n","lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","lstm_0 = LSTM(256)(lstm_1)\n","\n","\n","# lstm_0 = LSTM(512)(concat)\n","\n","# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n","# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","# lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","dropout = Dropout(drop)(lstm_0)\n","output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n","\n","# this creates a model that includes\n","model = Model(inputs, output)\n","\n","\n","adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]"]},{"cell_type":"markdown","metadata":{},"source":["#CNN MODEL"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1653989417080,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"njBANdn5mg-B","outputId":"24647910-9144-4987-83a2-fe64e64a04ac"},"outputs":[],"source":["# from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n","# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n","# from tensorflow.keras.models import Model\n","# from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n","# from tensorflow.keras.layers import Reshape, Flatten\n","# from tensorflow.keras.callbacks import EarlyStopping\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.models import Model\n","# from tensorflow.keras import regularizers\n","# sequence_length = data.shape[1]\n","# filter_sizes = [3,4,5]\n","# num_filters = 100\n","# drop = 0.5\n","\n","# inputs = Input(shape=(sequence_length,))\n","# embedding = embedding_layer(inputs)\n","# # reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n","\n","# conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n","# conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n","# conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n","# print(conv_1)\n","# maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n","# maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n","# maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n","\n","# merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n","# flatten = Flatten()(merged_tensor)\n","# reshape = Reshape((3*num_filters,))(flatten)\n","# dropout = Dropout(drop)(flatten)\n","# output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n","\n","\n","# # this creates a model that includes\n","# model = Model(inputs, output)\n","\n","\n","# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","# model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","# model.summary()\n","\n","# #define callbacks\n","# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","# callbacks_list = [early_stopping]\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10375,"status":"ok","timestamp":1653989547863,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"Jn0dBlzjmg-D","outputId":"be7de957-bf78-4fff-bc31-b5e586f705fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","32/32 [==============================] - 3037s 96s/step - loss: 0.5802 - accuracy: 0.8076 - val_loss: 1.6794 - val_accuracy: 0.0000e+00\n","Epoch 2/2\n","32/32 [==============================] - 5033s 159s/step - loss: 0.5065 - accuracy: 0.8331 - val_loss: 1.8499 - val_accuracy: 0.0000e+00\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x2016246e690>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(data, labels, validation_split=0.2,\n","          epochs=2, batch_size=128, callbacks=callbacks_list, shuffle=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"8XoN2UOamg-D"},"outputs":[],"source":["labels_test = data_test.iloc[:, 0].values\n","reviews_test = data_test.iloc[:, 1].values"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"PwiYb3Ohmg-E"},"outputs":[],"source":["encoded_labels_test = []\n","\n","for label_test in labels_test:\n","    if label_test == -1:\n","        encoded_labels_test.append([1,0,0])\n","    elif label_test == 0:\n","        encoded_labels_test.append([0,1,0])\n","    else:\n","        encoded_labels_test.append([0,0,1])\n","\n","encoded_labels_test = np.array(encoded_labels_test)  "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"E08tBw9img-E"},"outputs":[],"source":["reviews_processed_test = []\n","unlabeled_processed_test = [] \n","for review_test in reviews_test:\n","    review_cool_one = ''.join([char for char in review_test if char not in digits])\n","    reviews_processed_test.append(review_cool_one)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"OwgI9Xywmg-E"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews_test = []\n","all_words = []\n","for review_test in reviews_processed_test:\n","    review_test = ViTokenizer.tokenize(review_test.lower())\n","    word_reviews_test.append(review_test.split())"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"p02GxCh6mg-F"},"outputs":[],"source":["sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n","labels_test = encoded_labels_test"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1653989480441,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"jAqUMGInmg-F","outputId":"40acf056-b9c9-42ed-c767-423cc2054383"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (1050, 300)\n","Shape of label train and validation tensor: (1050, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data_test.shape)\n","print('Shape of label train and validation tensor:', labels_test.shape)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":596,"status":"ok","timestamp":1653989548454,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"LKclttiOmg-F","outputId":"6abe8e2e-7ed4-439f-8899-e6b30a044758"},"outputs":[{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 198s 6s/step - loss: 0.7581 - accuracy: 0.6667\n"]}],"source":["score = model.evaluate(data_test, labels_test)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1653989552805,"user":{"displayName":"Đức Nguyễn Quang","userId":"15255943122151670013"},"user_tz":-420},"id":"r31_uxxgmg-G","outputId":"2fb9d570-546b-4652-ca5f-5e6794fc6198"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 75.81%\n","accuracy: 66.67%\n"]}],"source":["print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"]}],"metadata":{"accelerator":"GPU","colab":{"name":"word2vec+cnn_v3.ipynb","provenance":[{"file_id":"1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ","timestamp":1653989613942}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
